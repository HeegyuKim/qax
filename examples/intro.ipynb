{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792ddd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import transformers\n",
    "import qax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a2559",
   "metadata": {},
   "source": [
    "# Qax: If it quacks like a tensor...\n",
    "[Qax](https://github.com/davisyoshida/qax) is a tool for implementing types which represent tensors, but aren't actually instantiated as a single dense array on your GPU. Examples of this include:\n",
    "* Quantization: A 4-bit array of integers + a small number of scale values are used to represent a full 16/32-bit array\n",
    "* LoRA: An array $W$ is replaced by the array $(W + BA^T)$ so that $A$ and $B$ may be trained while leaving $W$ frozen\n",
    "* Symbolic zeros/constants: For arrays which will consist entirely of a single repeated value, simply store that single value and the shape of the array\n",
    "* Custom kernels: If you have a custom kernel and want to use it with existing models without modifying them, Qax is an easy way to do so\n",
    "* Hopefully many more things!\n",
    "\n",
    "The goal of Qax is to make implementing custom JAX behavior much easier, so that users won't need to deal with all the details of writing a full JAX transform. All you need to do to get custom representations is:\n",
    "\n",
    "1. Write a function which describes how to construct a dense array from your representation\n",
    "2. Optionally write any number of handlers which specify how your arrays behave under JAX primitives such as multiplication\n",
    "\n",
    "Both of the above are written in pure JAX, so you don't need to implement any custom gradients or anything (unless you want to of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3330fa3",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Just run:\n",
    "```\n",
    "pip install qax\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6afedbf",
   "metadata": {},
   "source": [
    "## Example 1: A symbolic zero\n",
    "The way you specify custom behavior with Qax is to subclass the `qax.ImplicitArray` abstract class. One of the simplest things we could implement is a symbolic zero: A data type which represents an arbitrary tensor full of zeros without actually instantiating them on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497c6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zeros(qax.ImplicitArray):\n",
    "    def materialize(self):\n",
    "        print('Materializing Zeros!')\n",
    "        # self.shape and self.dtype will be\n",
    "        # populated by the ImplicitArray constructor\n",
    "        return jnp.zeros(self.shape, self.dtype)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'Zeros({self.shape}, {self.dtype})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9321d4",
   "metadata": {},
   "source": [
    "The only mandatory method to implement when subclassing `ImplicitArray` is `materialize()`.\n",
    "`materialize()` specifies how to turn our _implicitly_ represented array into an _explicit_ one, i.e. a single dense JAX array. In the case of `Zeros`, we can just call `jnp.zeros`.\n",
    "\n",
    "Let's instantiate a `Zeros` instance to try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c77926",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Zeros(shape=(2, 3), dtype=jnp.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c218b",
   "metadata": {},
   "source": [
    "By default JAX won't know how to use our new type. In order to use it in functions, we apply the `@use_implicit_args` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qax.use_implicit_args\n",
    "def f(x, y):\n",
    "    return (x + y)[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2696675",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f(z, jnp.ones(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae93a1",
   "metadata": {},
   "source": [
    "The cool thing is that `f` doesn't need to have any idea that it will be called with `ImplicitArray` instances, so we can use this with any pre-existing model. Right now this isn't much use, since all `z` is being materialized into a dense array as soon as it's needed for a JAX operation.\n",
    "\n",
    "To make our `Zeros` do something productive, let's implement the fact that $x + 0$ is always equal to $x$. We do this using the `@qax.primitive_handler` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binop_result_shape_dtype(a, b):\n",
    "    out_shape = jnp.broadcast_shapes(jnp.shape(a), jnp.shape(b))\n",
    "    out_dtype = jnp.result_type(a.dtype, b.dtype)\n",
    "    return out_shape, out_dtype\n",
    "\n",
    "# primitive_handler() takes a string, JAX primitive, or a list of those types\n",
    "# strings are used to find the corresponding primitive from `jax.lax`\n",
    "@qax.primitive_handler('add')\n",
    "def my_add_handler(primitive, a : Zeros, b):\n",
    "    # Handlers will receive as arguments:\n",
    "    # - primitive: a jax.core.Primitive instance (often can be ignored if the handler is just for one op)\n",
    "    # Any number of arguments which are either JAX values or ImplicitArrays\n",
    "    # Keyword arguments specifying parameters of the operation (e.g. axes for reduction operations)\n",
    "    \n",
    "    out_shape, out_dtype = get_binop_result_shape_dtype(a, b)\n",
    "    \n",
    "    if isinstance(b, Zeros):\n",
    "        # We can return further ImplicitArray instances if we want\n",
    "        return Zeros(out_shape, out_dtype)\n",
    "    \n",
    "    # Return b, possibly modifying its shape or dtype\n",
    "    return jnp.broadcast_to(b, out_shape).astype(out_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b173ce",
   "metadata": {},
   "source": [
    "The type annotation `a : Zeros` is actually important, Qax uses [Plum](https://github.com/beartype/plum) for multiple dispatch. You can even use this to define how different subclasses of ImplicitArray should interact with each other.\n",
    "\n",
    "(For convenience, commutative binary ops like $+$ and $\\times$ will automatically get their argument order switched so that the `ImplicitArray` instance comes first.)\n",
    "\n",
    "Now when we call `f`, we no longer see the materialization log message, since our add handler is skipping over ever instantiating the array of zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f(z, jnp.ones(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ab6e",
   "metadata": {},
   "source": [
    "Let's define a multiplication handler as well, since $x \\cdot 0 = 0$ for all $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8302f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qax.primitive_handler('mul')\n",
    "def handle_mul(primitive, a : Zeros, b):\n",
    "    out_shape, out_dtype = get_binop_result_shape_dtype(a, b)\n",
    "    \n",
    "    return Zeros(out_shape, out_dtype)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "@qax.use_implicit_args\n",
    "def g(x, y):\n",
    "    return (1 + x) * y\n",
    "\n",
    "print(g(z, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f6a67",
   "metadata": {},
   "source": [
    "The output of `use_implicit_args` is a function which is compatible with all the usual JAX transformations such as `jit`, `vmap`, `grad`, etc.\n",
    "\n",
    "Even this simple implementation is enough to let us modify the behavior of models which were written without knowing about Qax. Let's try replacing all the biases in HuggingFace's GPT-2 with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc726d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qax.primitive_handler('broadcast_in_dim')\n",
    "def broadcast(primitive, a : Zeros, *, shape, broadcast_dimensions):\n",
    "    # The biases get broadcast in order to add them to the activations\n",
    "    # so we need to handle that case\n",
    "    # Sometimes the simplest thing to do is use jax.eval_shape\n",
    "    # to figure out what shape to return\n",
    "    result_shape = jax.eval_shape(\n",
    "        partial(jax.lax.broadcast_in_dim, shape=shape, broadcast_dimensions=broadcast_dimensions),\n",
    "        a.aval # ImplicitArray has an aval property which will get an abstract shape/dtype\n",
    "    ).shape\n",
    "    return Zeros(result_shape, a.dtype)\n",
    "    \n",
    "\n",
    "model, params = transformers.FlaxAutoModelForCausalLM.from_pretrained('gpt2', _do_init=False)\n",
    "\n",
    "inputs = jnp.arange(1, 10)[None]\n",
    "\n",
    "# Helper function to switch all the biases\n",
    "# in the params out for some other value\n",
    "def replace_biases(params, replacer):\n",
    "    def maybe_replace_val(path, val):\n",
    "        if val.ndim != 1:\n",
    "            return val\n",
    "\n",
    "        # Skip layernorms\n",
    "        if any(\n",
    "            isinstance(p, jax.tree_util.DictKey) and p.key.startswith('ln')\n",
    "            for p in path\n",
    "        ):\n",
    "            return val\n",
    "        return replacer(val.shape, val.dtype)\n",
    "    return jax.tree_util.tree_map_with_path(maybe_replace_val, params)\n",
    "\n",
    "\n",
    "# Replace the biases with dense zero arrays:\n",
    "params_with_zeros = replace_biases(params, jnp.zeros)\n",
    "print('New bias:', params['transformer']['h']['0']['attn']['c_attn']['bias'])\n",
    "\n",
    "output = model(inputs, params=params_with_zeros).logits\n",
    "print('Last logit average:', jnp.mean(output[0, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e43cc4",
   "metadata": {},
   "source": [
    "Now let's try replacing them with our symbolic zeros instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3135f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_with_zeros = replace_biases(params, Zeros)\n",
    "print('New bias:', params['transformer']['h']['0']['attn']['c_attn']['bias'])\n",
    "\n",
    "# In this case since we're calling the model directly, we need to\n",
    "# wrap it so we can pass params in a positional argument\n",
    "# This usually won't be an issue since the call to the model will\n",
    "# be inside a loss function or some other function\n",
    "\n",
    "output = qax.use_implicit_args(model)(inputs, params=params_with_zeros).logits\n",
    "print('Last logit average:', jnp.mean(output[0, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f6dc7",
   "metadata": {},
   "source": [
    "We got the same result, but using 0 FLOPs for adding the biases! If you really wanted to flesh out the behavior of `Zeros`, you could also add handlers defining its output for primitives such as `sin`, `cos`, etc. Let's move on to something more interesting though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1bb4a5",
   "metadata": {},
   "source": [
    "## Example 2: LoRA\n",
    "In this example we'll implement [LoRA](https://arxiv.org/abs/2106.09685) in just a few lines of code. Unlike the `Zeros` example from the previous section, our `ImplicitArray` subclass will actually contain data this time. As such we'll need to implement flattening/unflattening logic, since all `ImplicitArray` subclasses are pytrees. This also means you can use `tree_map` and friends to manipulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603053f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraMatrix(qax.ImplicitArray):\n",
    "    \"\"\"Represent W + A B^T\"\"\"\n",
    "    def __init__(self, w, a, b):\n",
    "        assert w.aval.ndim == a.aval.ndim == b.aval.ndim == 2\n",
    "        assert a.shape[1] == b.shape[1]\n",
    "        assert a.shape[0] == w.shape[0]\n",
    "        assert b.shape[0] == w.shape[1]\n",
    "        assert a.dtype == b.dtype == w.dtype\n",
    "        super().__init__(w.shape, w.dtype)\n",
    "\n",
    "        self.w = w\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def flatten(self):\n",
    "        \"\"\"\n",
    "        Return a tuple of (children, auxiliary_data)\n",
    "        Each element in `children` should be a (key, value) tuple\n",
    "        \"\"\"\n",
    "        children = [\n",
    "            ('w', self.w),\n",
    "            ('a', self.a),\n",
    "            ('b', self.b)\n",
    "        ]\n",
    "\n",
    "        # The shape/dtype are handled by the base class,\n",
    "        # so we don't need to worry about flattening them\n",
    "        aux_data = ()\n",
    "        return children, aux_data\n",
    "\n",
    "    def unflatten(self, aux_data, children):\n",
    "        \"\"\"\n",
    "        Unflatten will be called on an _uninitialized_ LoraMatrix\n",
    "        We need to use aux_data and children to repopulate its\n",
    "        members.\n",
    "        \"\"\"\n",
    "        self.w, self.a, self.b = children\n",
    "\n",
    "    def materialize(self):\n",
    "        print('Materializing LoraMatrix!')\n",
    "        return self.w + self.b.T @ self.a\n",
    "\n",
    "\n",
    "@qax.primitive_handler('dot_general')\n",
    "def f(primitive, x : jax.Array, w : LoraMatrix, *, dimension_numbers, **kwargs):\n",
    "    # For this example, we'll only handle the simple case of of x @ w, rather than\n",
    "    # all possible dot_general invocations\n",
    "    # We're also completely ignoring the `precision` and `preferred_element_type`\n",
    "    # args. A full implementation would use those as well.\n",
    "    (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers\n",
    "    # This check just makes sure\n",
    "    # that all that's happening is a simple matmul\n",
    "    if not (\n",
    "        len(w.shape) == 2\n",
    "        and lhs_contract == (x.ndim - 1,)\n",
    "        and rhs_contract == (0,)\n",
    "        and lhs_batch == ()\n",
    "        and rhs_batch == ()\n",
    "    ):\n",
    "        # If we want to only partially handle a particular primitive,\n",
    "        # we can fall back to the default logic by returning NotImplemented\n",
    "        return NotImplemented\n",
    "\n",
    "    result = x @ w.w\n",
    "    result += x @ w.a @ w.b.T\n",
    "    return result\n",
    "\n",
    "def lora_from_tree(tree, key, lora_dim=8):\n",
    "    def iter_keys(key):\n",
    "        while True:\n",
    "            key, k2 = jax.random.split(key)\n",
    "            yield k2\n",
    "            \n",
    "    key_it = iter_keys(key)\n",
    "    def map_fn(path, val):\n",
    "        if val.ndim != 2:\n",
    "            return val\n",
    "        \n",
    "        # Skip embedding params\n",
    "        if any(isinstance(p, jax.tree_util.DictKey) and p.key == 'embedding' for p in path):\n",
    "            return val\n",
    "        \n",
    "        a = jax.random.normal(next(key_it), (val.shape[0], lora_dim), val.dtype)\n",
    "        b = jnp.zeros((val.shape[1], lora_dim), val.dtype)\n",
    "        return LoraMatrix(val, a, b)       \n",
    "    \n",
    "    return jax.tree_util.tree_map_with_path(map_fn, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee1d5f",
   "metadata": {},
   "source": [
    "Let's try it out on a T5 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c697f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5, params = transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained('t5-small', _do_init=False)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('t5-small')\n",
    "encoder_inputs = jnp.asarray(tokenizer.encode('Some input'))[None]\n",
    "decoder_inputs = jnp.asarray([0] + tokenizer.encode('Some output'))[None]\n",
    "\n",
    "lora_params = lora_from_tree(params, jax.random.PRNGKey(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed426e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_output = t5(input_ids=encoder_inputs, decoder_input_ids=decoder_inputs, params=params).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_output = qax.use_implicit_args(t5)(\n",
    "    input_ids=encoder_inputs,\n",
    "    decoder_input_ids=decoder_inputs,\n",
    "    params=lora_params\n",
    ").logits\n",
    "print(jnp.max(jnp.abs(lora_output - orig_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290754e7",
   "metadata": {},
   "source": [
    "The LoRA result is identical to the execution of the unmodified network, and we didn't get any materialization warnings so we successfully made a LoRA forward pass without ever calculating $W + AB^T$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149cfaa",
   "metadata": {},
   "source": [
    "## Training\n",
    "So far we haven't looked at how to train a model when using Qax. The main thing to understand is that you should apply `qax.use_implicit_args` first, _then_ differentiate the resulting function. `use_implicit_args` transforms the function into one which goes from pytrees to pytrees, so all the standard JAX autodiff machinery will work.\n",
    "\n",
    "If you need to update only a subset of the elements of an ImplicitArray instance (e.g. only `a` and `b` for LoRA), Qax provides `qax.utils.freeze_qax_keys` to make this easier. Here's an end-to-end example training T5 to memorize the input/output pair from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1af466",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(3e-4)\n",
    "# freeze_keys_in_optimizer takes an optax optimizer, the ImplicitArray subclass to freeze for, \n",
    "# and an iterable of the keys to be frozen\n",
    "optimizer = qax.utils.freeze_qax_keys(optimizer, LoraMatrix, ['w'])\n",
    "\n",
    "# We're only using a single example so we'll just close over the training data\n",
    "# There are no code changes from an ordinary training loop other than decorating\n",
    "# loss_fn with @use_implicit_args\n",
    "\n",
    "@qax.use_implicit_args\n",
    "def loss_fn(params):\n",
    "    decoder_ids = decoder_inputs[:, :-1]\n",
    "    targets = decoder_inputs[:, 1:]\n",
    "    logits = t5(\n",
    "        input_ids=encoder_inputs,\n",
    "        decoder_input_ids=decoder_ids,\n",
    "        params=params\n",
    "    ).logits\n",
    "    \n",
    "    logprobs = jax.nn.log_softmax(logits)\n",
    "    target_logprobs = jnp.take_along_axis(logprobs, targets[:, :, None], axis=-1)\n",
    "    loss = -jnp.sum(target_logprobs)\n",
    "    return loss\n",
    "\n",
    "grad_fn = jax.value_and_grad(loss_fn)\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state):\n",
    "    loss, grads = grad_fn(params)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state, params=params)\n",
    "    new_params = optax.apply_updates(updates, params)\n",
    "    return loss, new_params, new_opt_state\n",
    "\n",
    "opt_state = optimizer.init(lora_params)\n",
    "for step in range(20):\n",
    "    loss, lora_params, opt_state = update(lora_params, opt_state)\n",
    "    print(f'{step}. {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0045a3",
   "metadata": {},
   "source": [
    "That's all you need to know to get started using Qax!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddccd660",
   "metadata": {},
   "source": [
    "## Example 3: Nesting\n",
    "Qax supports arbitrary nesting of `ImplicitArray` instances without. Here's a quick demo combining the previous two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qax.use_implicit_args\n",
    "def g(w, x):\n",
    "    return jnp.sum(x @ w)\n",
    "\n",
    "w = jnp.ones((3, 5))\n",
    "x = jnp.arange(3, dtype=jnp.float32)\n",
    "\n",
    "lora_with_symbolic_zero = LoraMatrix(\n",
    "    w=w,\n",
    "    a=Zeros((w.shape[0], 6), jnp.float32),\n",
    "    b=Zeros((w.shape[1], 6), jnp.float32)\n",
    ")\n",
    "print(f'Original: {g(w, x)}')\n",
    "print(f'With lora: {g(lora_with_symbolic_zero, x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65628ce",
   "metadata": {},
   "source": [
    "If we wanted we could write a `dot_general` handler to avoid the materialization as well, but the main point is just to illustrate that it's easy to mix and match different `ImplicitArray` subclasses. A more useful example might be using a symbolic zero as the offset for a quantization datatypes which expects both an offset and a scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51826e2",
   "metadata": {},
   "source": [
    "## Other examples\n",
    "[Here's](https://github.com/davisyoshida/abnormal-floats/blob/8421c0b4677c5cd8e527ae999b00ee4a629c3318/transform.py#L17) an example of using Qax to implement a 4-bit quantized matrix representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
